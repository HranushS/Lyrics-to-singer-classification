# -*- coding: utf-8 -*-
"""NN_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uFO1XCWqBaNh7y6XM1CaK8HbtDNTxeQU
"""

# import libraries
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import re
import random
import nltk
from scipy import sparse
from scipy.sparse import csr_matrix, vstack
from textblob import TextBlob
import pickle
from datetime import datetime

from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score
from tensorflow import keras
from keras import layers
from keras.layers import TextVectorization
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('../Datasets/translated.csv')
df.head()

def preprocessing_func(df):
    """
    Perform preprocessing steps on a DataFrame.

    Args:
        df (pandas.DataFrame): The DataFrame to be preprocessed.

    Returns:
        pandas.DataFrame: The preprocessed DataFrame.

    """
    # Remove rows with missing values
    df.dropna(inplace=True)

    # Drop a specific row by index
    df.drop(265, inplace=True)

    # Replace newline, carriage return, and tab characters with a space
    df['Translated_Lyrics'] = df['Translated_Lyrics'].str.replace(r'\n|\r|\t', ' ', regex=True)

    # Rename columns
    df.rename(columns={'Artist': 'artist', 'Translated_Lyrics': 'text', 'Song': 'song'}, inplace=True)

    return df

def stem_from_tokens(df, column_name):
    """
    Apply stemming to tokens in a DataFrame column.

    Args:
        df (pandas.DataFrame): The DataFrame containing the column with tokens.
        column_name (str): The name of the column containing the tokens.

    Returns:
        pandas.DataFrame: The DataFrame with an additional column containing the stemmed tokens.

    """
    # Initialize the stemmer
    stemmer = nltk.stem.porter.PorterStemmer()
    token_to_stem = {}
    token_count = 0  # Initialize word count

    # Iterate through all songs
    for lst in df[column_name]:
        # Iterate through all tokens of a song
        for token in lst:
            token_count += 1
            # Check if token is in the dictionary
            if token not in token_to_stem:
                # Add token to the dictionary with its stem
                token_to_stem[token] = stemmer.stem(token)

    # Map the tokens to their stems in a new column
    df['stems'] = df[column_name].map(lambda lst: [token_to_stem[token] for token in lst])
    df['stems_str'] = df['stems'].map(lambda lst: ' '.join(lst))

    return df

def encode_labels(df, label_column):
    """
    Encode labels in a DataFrame column using LabelEncoder.

    Args:
        df (pandas.DataFrame): The DataFrame containing the label column.
        label_column (str): The name of the column containing the labels.

    Returns:
        pandas.DataFrame: The DataFrame with an additional column containing the encoded labels.

    """
    label_encoder = LabelEncoder()
    df['target'] = label_encoder.fit_transform(df[label_column])
    return df

def make_bached_dataset(batch_size, df):
    """
    Create batched TensorFlow datasets for training and testing.

    Args:
        batch_size (int): The batch size for the datasets.
        df (pandas.DataFrame): The DataFrame containing the dataset.

    Returns:
        tuple: A tuple containing the training and testing datasets.
               Each dataset is a TensorFlow dataset object.

    """
    targets = df['target'].to_numpy()

    train_idx, valid_idx = train_test_split(
        np.arange(len(targets)),
        test_size=0.2,
        shuffle=True,
        stratify=targets
    )

    train = df.iloc[train_idx]
    test = df.iloc[valid_idx]

    X_train = train["stems_str"]
    y_train = train["target"]
    X_test = test["stems_str"]
    y_test = test["target"]

    # Create TensorFlow datasets from X_train and y_train
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))

    # Batch the datasets
    train_dataset = train_dataset.batch(batch_size)
    test_dataset = test_dataset.batch(batch_size)

    return train_dataset, test_dataset, y_test

def make_vectorized_dataset(max_tokens, train_dataset, test_dataset):
    """
    Create a vectorized dataset using TextVectorization.

    Args:
        max_tokens (int): The maximum number of tokens to consider in the vectorization process.
        train_dataset (tf.data.Dataset): The training dataset.
        test_dataset (tf.data.Dataset): The testing dataset.

    Returns:
        tuple: A tuple containing the vectorized training and testing datasets.

    """
    text_vectorization = TextVectorization(
        max_tokens=max_tokens,
        output_mode="multi_hot"
    )

    text_only_train_ds = train_dataset.map(lambda x, y: x)
    text_vectorization.adapt(text_only_train_ds)

    train_ds = train_dataset.map(
        lambda x, y: (text_vectorization(x), y),
        num_parallel_calls=4
    )

    test_ds = test_dataset.map(
        lambda x, y: (text_vectorization(x), y),
        num_parallel_calls=4
    )

    return train_ds, test_ds

def get_model(max_tokens):
    """
    Create a neural network model.

    Args:
        max_tokens (int): The number of input tokens.

    Returns:
        tensorflow.keras.Model: The compiled neural network model.

    """
    inputs = keras.Input(shape=(max_tokens,))
    features = layers.Dense(256, activation="relu")(inputs)
    features = layers.Dense(128, activation="relu")(features)
    features = layers.Dense(64, activation="relu")(features)
    outputs = layers.Dense(20, activation="softmax")(features)
    model = keras.Model(inputs=inputs, outputs=outputs)

    model.compile(
        optimizer="rmsprop",
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )

    return model

def model_fit(model, train_ds, test_ds, path):
    """
    Fit a neural network model to the training dataset and evaluate it on the testing dataset.

    Args:
        train_ds (tf.data.Dataset): The training dataset.
        test_ds (tf.data.Dataset): The testing dataset.
        path (str): The path to save the best model checkpoint.

    Returns:
        tuple: A tuple containing the trained model and the accuracy string.

    """
    
    callbacks = [
        keras.callbacks.ModelCheckpoint(path, save_best_only=True)
    ]

    model.fit(
        train_ds.cache(),
        validation_data=test_ds.cache(),
        epochs=15,
        callbacks=callbacks
    )

    model = keras.models.load_model(path)
    accuracy = f"Test acc: {model.evaluate(test_ds)[1]:.3f}"
    return model, accuracy

def load_model(path):
    """
    Load a saved neural network model.

    Args:
        path (str): The path to the saved model.

    Returns:
        tensorflow.keras.Model: The loaded model.

    """
    model = keras.models.load_model(path)
    return model

def model_predict(model, test_ds):
    """
    Make predictions using a trained neural network model.

    Args:
        model (tensorflow.keras.Model): The trained model.
        test_ds (tf.data.Dataset): The dataset to make predictions on.

    Returns:
        numpy.ndarray: The predicted labels.

    """
    y_pred = np.argmax(model.predict(test_ds), axis=1)
    return y_pred

#Preprocess the data
preprocessing_func(df)

#Tokenize text column
tokenize_column(df, 'text' , language='en')

#Stemming
stem_from_tokens(df,'tokens_en')

#Drop unnecessary columns
df.drop(["text", "tokens_en", "song", "stems"], axis=1, inplace=True)

#Encode the target
encode_labels(df, 'artist')

#Make a dataset
batch_size = 8
train_dataset, test_dataset, y_test = make_bached_dataset(batch_size, df)

#Vectorize text data
max_tokens = 5000
train_ds, test_ds = make_vectorized_dataset(max_tokens, train_dataset, test_dataset)

#Build a model
model = get_model(max_tokens)

#Fit the model to data and load the best model
path = 'dense_layers.keras'
best_model, accuracy = model_fit(model,train_ds, test_ds, path)

#Make predictions
y_pred = model_predict(best_model, test_ds)

#Test accuracy
accuracy

# Calculate the confusion matrix
confusion_matrix = tf.math.confusion_matrix(labels=y_test, predictions=model_predict(model, test_ds))

# Print the confusion matrix
print(confusion_matrix)

# Compute F1 score
f1 = f1_score(y_test, y_pred, average='weighted') 

print("F1 Score:", f1)